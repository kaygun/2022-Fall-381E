{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1293add-4813-4f9a-a81d-f4457c6fcbcb",
   "metadata": {},
   "source": [
    "# MAT 381E Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e2430-64c2-467d-8cd5-7d50284fb6a7",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "1. Using the library [Beautiful Soup](https://pypi.org/project/beautifulsoup4/) write a function that takes a URL from a specific Turkish Newspaper (any newspaper you'd like) and extracts and then returns the text (not the HTML source) of the news article given in the URL. (Warning: you cannot write a universal function that works for every newspaper. The function has to be specific to a specific newspaper.)\n",
    "\n",
    "2. Write a separate function that cleans the text and removes all Turkish stopwords for a text coming from Step 1.\n",
    "\n",
    "3. Display word clouds of 5 news articles after you apply the functions in Step 1 and Step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa844706-5f9d-4db2-b85d-50bf4174c5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be6dba34-dcce-44f6-b32a-c7dca064d781",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "1. Using the library [tweepy](https://docs.tweepy.org/en/stable/index.html) pull 5000 tweets (in English) on any topic you'd like.\n",
    "\n",
    "2. Clean the text and remove all stopwords. Then sketch a word cloud of your collection.\n",
    "\n",
    "2. Using the library [NLTK](https://www.nltk.org/) apply sentiment analysis on the tweets you collected. \n",
    "\n",
    "3. Analyze your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f525639-5aa1-417c-a450-7123eb09ca5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec6b4daf-2a32-4ca3-bfae-d8043a3f0825",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "For this question use [UFO Sightings Dataset](https://www.kaggle.com/code/hakeemtfrank/ufo-sightings-data-exploration/data) from Kaggle.\n",
    "\n",
    "1. Ingest the dataset as a pandas dataframe, and clean it if necessary.\n",
    "\n",
    "2. Plot a geographic heatmap of UFO sightings and find hotspots.\n",
    "\n",
    "3. Is there a statistically verifiable correlation between shape of the UFO and the duration of the sighting. Investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c046bd2-0e3e-4fe2-aeb2-519e94795bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4db1cd26-017e-4c94-9824-881f8c7646c7",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "For this question use [Turkish Institute of Statistics (TÜİK)](https://data.tuik.gov.tr/) data portal, and [The Humanitarian Data Exchange (HDX)](https://data.humdata.org/) data portal.\n",
    "\n",
    "1. Get the population data for cities in Turkey as a pandas data frame from TUIK. Notice that TUIK generates the data only for human consumption. Clean the data and get well-defined columns and rows that contain only the data removing all unnecessary metadata (keep the column names). Cleaning must be done using python only. You are NOT allowed to clean it externally.\n",
    "\n",
    "2. Get the crime statistics (Suçun İşlendiği İl ve Suç Türüne Göre Ceza İnfaz Kurumundan Çıkan Hükümlüler) as a pandas data frame from TUIK. Clean the data and get well-defined columns and rows that contain only the data removing all unnecessary metadata (keep the column names). Cleaning must be done python only. You are NOT allowed to clean it externally.\n",
    "\n",
    "3. Get the shape files for the Turkish city municipality borders from HDX, ingest it as a geoPandas datafroma, and then clean it if necessary. Cleaning must be done using python only. You are NOT allowed to clean it externally.\n",
    "\n",
    "4. Merge the population data, crime data, and the shapefile data into a single geoPandas dataframe.\n",
    "\n",
    "5. Plot the population data as a choropleth map as I did in the class.\n",
    "\n",
    "6. Plot the crime data for different categories. However, don't use the raw numbers. Use the normalized data with respect to the population of the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febfc653-fe81-4051-aa79-e883af763bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
